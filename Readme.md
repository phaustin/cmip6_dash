# installing

1) install conda-lock

       conda install conda-lock mamba

2) generate a new conda lock file if environment.yml has changed:

       conda-lock -f environment.yml -p linux-64

   or win-64 or macos-64

3) create and activate the new environment:

      mamba create --name dash --file conda-linux-64.lock
      mamba activate dash
      pip install -r requirements.txt

4) start the app and browse localhost:8050

     python app.py

5) to install the pre-commit hooks

     pre-commit install

6) to check in ignoring the pre-commit failures

     git commit --no-verify

## Dashboard Rundown
![Dash Landing Page](docs/CMIP_6_heatmap_numbered.png)
![Dash Comaprison Hist Page](docs/CMIP_6_probability_density_comparison_nums.png)
![Dash Member_Comparisons](docs/CMIP_6_member_comparison_nums.png)

1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.

## Adding cases

1) Adding a case can be achieved with the following commands. Jupyter lab was initially used for case development but really any python interactive evironment would work. Using the "none" case selection in a local instance of the dashboard is also useful for case design.

     ``` Python
     write_case_definition(<specifications>),
     with open((case_file_path + <name_of_case_file>)) as f:
          data = json.load(f)
     get_case_data(<specifications>))
     ```

2) After you are happy with the case, the code should be transfered to make_case.py and version controlled. A directory will be created in the cases/ file corresponding to the name of the scenario. Each .nc file will contain all the member runs for the different combinations of models and variables.

3) Cases can be regenerated by calling python make_cases.py from the root of the directory

### Case troubleshooting

If case creation fails, the most likley cause is that the query generated by get_case_data() is incorrect.

In most cases, the code generating the case definition will throw a meaningful error before the time consuming data downloading actually takes place but the input validation is not currently exhaustive. If the function fails and doesn't thow a useful error You can use https://docs.google.com/document/d/1yUx6jr9EdedCOLd--CPdTfGDwEwzPpCF6p1jRmqx-0Q/edit?usp=sharing to check that the dates, variables, models, and experiments are compatible.

If the case succeeds in running but isn't working on the dasboard, check that the name of the json is not different from the scenario name and that the corresponding folder exists for the json file.

## For Developers

This section of the readme provides some guidance for adding new features or fixing bugs.

### git commit hooks

If you are running into issues with the git hooks, ensure that black and flake8 aren't disagreeing on something. Flake8 issues can be introduced by the black linter. Its typically easier to change flake8's preferences rather than black's. See .flake8 in the root directory for how to deal with this issue.
### Repository Structure

There are a few oddities in the structure of this repo. The tests currently live in the main directory, as well as the script for creating cases.
### Tests

There are tests for some of the case_util functions and some of the wrangling functions. Regression testing for the visuals and integration testing
with something like selenium for the actual dashboard code has not been implemented. You can invoke these tests by calling: `pytest` from the main directory.

They take about 3-5 minutes to run and are far from exhaustive but are worth running if you are making changes to the wrangling or the cases code.
### A note about cases vs. "Auto" mode

Design choices were mostly made with the idea that the dashboard would be used by students in "case" mode. The intention is that the option "none" would be removed when the class actually uses the tool and as such the dashboard is rather brittle in "none" mode. Better error handling and restricting available options to prevent incompatible input will probably required if the dashboard is to be run in production in "none" mode.

### I want to...

Here are some pointers on what code to look at specifically for adding features.
#### Add support for a new model / variable / experiment:

Adding support for more of the wide and wonderful world of CMIP-6 should be fairly straight forward. A few gotchas to lookout for are dates and gridding. Different scenarios, of course, have different dates but piControl rather charmingly does not always have the same dates between different models which can cause issues. The plotting code in the dashboard has only been tested with the gridding used in the Lmon and Amon tables- SImon, for example, causes issues.

Getting a new variable to show up as an option in "None" mode can be achieved by adding it to the dict returned by get_var_key(). Both get_mod_key() and the model options creation at the start of app.py would need to be changed for adding a new model and new experiment.
#### Add support for a new dashboard feature:

Graphics are currently created by functions in src/plotting_utils.py and called in app.py. Dasboard features are defined as variables with descriptive names and then put together in the app initilization call to avoid very long lines.

#### Change how cases are structured

Currently, cases can take an arbitrary number of variables and models, but are constrained to one experiment that must be valid for all models and variables. Two sets of dates may become necessary if this changes. The case creating functions live in src/case_utils.py.
